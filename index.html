<!DOCTYPE html>
<!-- Coding by CodingNepal || www.codingnepalweb.com -->
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Boxicons CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
    <link href="https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css" rel="stylesheet" />
    <title>Quality and Trust Solution Documentation</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- navbar -->
    <nav class="navbar">
      <div class="logo_item">
        <i class="fa-solid fa-bars" id="sidebarOpen"></i>
        <!-- <img src="images/logo.png" alt=""></i> -->
        Quality and Trust Solution
      </div>

      <div class="search_bar">
        <!-- <input type="text" placeholder="Search" /> -->
      </div>

      <div class="navbar_content">
        <i class="bi bi-grid"></i>
        <i class="fa-solid fa-sun" id="darkLight"></i>
        <img src="images/profile.jpg" alt="" class="profile" />
      </div>
    </nav>

    <!-- sidebar -->
    <nav class="sidebar">
      <div class="menu_content">
        <ul class="menu_items">
          

          <div class="menu_title menu_dahsboard"></div>
          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-house"></i>
              </span>
              <span class="navlink">Home</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink qts" onClick="clickCalled('qts')">Quality & Trust Solution</a>
              <a href="#" class="nav_link sublink intro" onClick="clickCalled('intro')">Introduction</a>
              <a href="#" class="nav_link sublink busRel" onClick="clickCalled('busRel')">Business Relevance</a>
              <a href="#" class="nav_link sublink relUC" onClick="clickCalled('relUC')">Relevant Use cases</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-screwdriver-wrench"></i>
              </span>
              <span class="navlink">Installation</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink preReq" onClick="clickCalled('preReq')">Prerequisite</a>
              <a href="#" class="nav_link sublink packIns" onClick="clickCalled('packIns')">Package Installation</a>
              <a href="#" class="nav_link sublink errRes" onClick="clickCalled('errRes')">Error Resolution</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-sliders"></i>
              </span>
              <span class="navlink">Configuration</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink llm" onClick="clickCalled('llm')">LLM Model Config</a>
              <a href="#" class="nav_link sublink dbConf" onClick="clickCalled('dbConf')">Database Config:</a>
              <a href="#" class="nav_link sublink setDb" onClick="clickCalled('setDb')">Setup database & use Lib</a>
              <a href="#" class="nav_link sublink mmc" onClick="clickCalled('mmc')">Metric/Metadata Config</a>
            </ul>
          </li>
          <!-- end -->
        </ul>

        <ul class="menu_items">
          <div class="menu_title menu_editor"></div>
          <!-- duplicate these li tag if you want to add or remove navlink only -->
          <!-- Start -->
          <li class="item">
            <a href="#" class="nav_link nav_res">
              <span class="navlink_icon">
                <i class="fa-solid fa-reply"></i>
              </span>
              <span class="navlink">Response</span>
            </a>
          </li>
          <!-- End -->

          <li class="item">
            <a href="#" class="nav_link">
              <span class="navlink_icon">
                <i class="fa-solid fa-magnifying-glass"></i>
              </span>
              <span class="navlink">Search</span>
            </a>
          </li>
          <li class="item">
            <a href="#" class="nav_link">
              <span class="navlink_icon">
                <i class="fa-solid fa-file-circle-check"></i>
              </span>
              <span class="navlink">Data Quality</span>
            </a>
          </li>
          <li class="item">
            <a href="#" class="nav_link">
              <span class="navlink_icon">
                <i class="fa-solid fa-diagram-project"></i>
              </span>
              <span class="navlink">Classification</span>
            </a>
          </li>
          <li class="item">
            <a href="#" class="nav_link">
              <span class="navlink_icon">
                </i><i class="fa-solid fa-flask-vial"></i>
              </span>
              <span class="navlink">Experiment Tracking</span>
            </a>
          </li>
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_model_benchmarking"></div>
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_ref"></div>
        </ul>

        <!-- Sidebar Open / Close -->
        <!-- <div class="bottom_content">
          <div class="bottom expand_sidebar">
            <span> Expand</span>
            <i class='bx bx-log-in' ></i>
          </div>
          <div class="bottom collapse_sidebar">
            <span> Collapse</span>
            <i class='bx bx-log-out'></i>
          </div>
        </div> -->
      </div>
    </nav>
    <div id="maincontent">
      <div id="default">
        CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="qts">
        CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="intro">
        <b>Citiustech Healthcare GENAI Quality & Trust (Q&T) Framework</b> is a software-based solution designed to ensure the quality, reliability, and trustworthiness of Generative AI (GENAI) applications in the healthcare industry. As organizations increasingly adopt AI-driven solutions, particularly Large Language Models (LLMs), it becomes crucial to monitor and assess their performance. The Q&T Framework is built to evaluate applications that leverage LLMs for critical tasks like question answering, summarization, classification, Named Entity Recognition (NER), and entity extraction.
        <br><br> framework is pivotal for organizations aiming to implement and scale GENAI applications within healthcare ecosystems. By integrating the Q&T Framework into the Continuous Integration/Continuous Deployment (CI/CD) pipeline, healthcare organizations can monitor, improve, and optimize the quality of AI-generated outputs, ensuring regulatory compliance, enhanced accuracy, and ultimately better patient outcomes.        
      </div>

      <div id="busRel">Generative AI is revolutionizing healthcare, from automating administrative tasks to assisting with clinical decision-making. However, its successful enterprise-wide adoption hinges on ensuring the quality, accuracy, and trustworthiness of AI-driven applications. The Citiustech Healthcare GENAI Quality & Trust Framework addresses this need by providing a structured, evaluative approach that organizations can use to: <br>
	• <b>Mitigate Risks:</b> Ensure that AI models are accurate, transparent, and ethical, thereby mitigating risks associated with incorrect or biased outputs in healthcare scenarios.
	• <b>Enhance Trust:</b> Build trust among healthcare providers and patients by ensuring that AI-driven decisions are reliable and aligned with healthcare standards and regulations.
	• <b>Optimize Performance:</b> The framework evaluates model effectiveness during development and post-deployment, improving performance in real time by identifying areas for prompt enhancement and error correction.
	• <b>Accelerate Scaling:</b> By integrating the framework into CI/CD pipelines, organizations can streamline the development and deployment process, facilitating large-scale adoption of GENAI applications across multiple healthcare use cases.
  <br>With a focus on tasks like question answering, summarization, classification, NER, entity extraction, and RAG (Retrieval-Augmented Generation) applications, the Q&T Framework supports ongoing AI model refinement, making it a vital tool for scaling AI innovations in healthcare.
      </div>

      <div id="relUC">
<i>1. Question Answering for Clinical Decision Support</i>
      <b>• Challenge:</b> Healthcare professionals often rely on AI models to provide fast, accurate answers to complex medical questions based on large datasets of clinical knowledge. Ensuring the quality and reliability of these answers is critical for patient safety.
      <b>• Solution:</b> The Q&T Framework evaluates the effectiveness of LLMs in clinical question-answering tasks, ensuring that responses are accurate, relevant, and free from bias. Continuous evaluation helps refine model responses over time, improving clinical decision-making.
<br><i>2. Summarization of Medical Records</i>
      <b>• Challenge:</b> Healthcare providers require concise, accurate summaries of extensive patient medical records, which are often generated by AI systems. Errors in summarization can lead to missed diagnoses or incorrect treatment plans.
      <b>• Solution:</b> The Q&T Framework assesses the quality of summarizations produced by GENAI models, ensuring the summaries are clear, comprehensive, and medically accurate. The evaluation results can be used to fine-tune summarization models to better meet clinical requirements.
<br><i>3. Classification of Medical Data</i>
      <b>• Challenge:</b> In applications like diagnostic tools and medical billing, AI models must accurately classify medical data (e.g., images, clinical notes) into predefined categories. Misclassification can result in incorrect diagnoses or billing errors.
      <b>• Solution:</b> The Q&T Framework evaluates the precision, recall, and overall accuracy of classification models, allowing healthcare organizations to monitor and improve model performance, ensuring data is classified correctly.
<br><i>4. Named Entity Recognition (NER) and Entity Extraction from Clinical Texts</i>
      <b>• Challenge:</b> AI systems in healthcare must accurately identify medical entities (e.g., diseases, drugs, symptoms) from unstructured clinical notes or research papers. Errors in NER or entity extraction can lead to incomplete patient data or incorrect analytics.
      <b>• Solution:</b> The Q&T Framework assesses the quality of NER and entity extraction tasks, ensuring that models accurately recognize and extract relevant entities from text. By integrating this into the CI/CD pipeline, developers can continuously improve the performance of NER models.
<br><i>5. Search and Retrieval for RAG (Retrieval-Augmented Generation) Applications</i>
      <b>• Challenge:</b> In RAG applications, LLMs rely on search and retrieval functions to generate relevant and accurate responses. Poor search results can lead to incomplete or incorrect AI outputs.
      <b>• Solution:</b> The Q&T Framework evaluates the effectiveness of the search and retrieval process in RAG applications. By continuously monitoring this aspect, the framework ensures that LLMs retrieve the most relevant information, improving overall response quality in GENAI applications.

How the Q&T Framework Supports Development and Scaling
      <b>• Reference-Free Evaluation:</b> The Q&T Framework can perform reference-free evaluations of specific components in the RAG pipeline. This means that the framework can assess model outputs without needing predefined correct answers, making it versatile in evaluating complex, open-ended tasks such as summarization or question answering.
      <b>• CI/CD Pipeline Integration:</b> By incorporating evaluation metrics into the CI/CD pipelines, developers can automatically monitor and improve the quality of entity extraction, response quality, and search performance as new iterations of the AI model are deployed. This real-time feedback mechanism ensures that healthcare AI applications continuously evolve and improve in terms of accuracy, relevance, and trustworthiness.
      <b>• Metrics for Trust and Quality:</b> The framework provides actionable metrics that help developers assess the quality and trust of the AI applications they are building. These metrics can drive prompt engineering, improving model performance at every stage of the AI lifecycle.
      </div>

      <div id="preReq">
        • Python Version 3.10.* to 3.11.*
	
        • MySQL Workbench
        
        • Postgres
        
        • Environment variables to be set and configured. Refer to Configurations for further steps.
      </div>

      <div id="packIns">
        <b>Steps:-</b>

          <b>Step 1:</b> Get access to Quality & Trust gitlab repository. Download Q&T package wheel file from https://git/generativeai/genai-trust-framework/-/tree/QT_backend/dist
          
          <b>Step 2:</b> For Package Installation, create a new virtual environment and activate it or install the package in existing application environment.
          
          <b>Step 3:</b> To install wheel file, execute below command -
          <b>pip install qualitytrust-1.0-py3-none-any.whl</b>
 
        All the libraries will be installed with dependencies.
 
        <b>Note -</b> if you face any issue while installation, please refer to error resolution page (link).
      </div>

      <div id="errRes">
        Error : Visual Build Tool error
        Solution : Download and install visual C++ build tool
        
        Error : Python package dependency errors
        Solution : Downgrade/upgrade common packages.        
      </div>

      <div id="llm">
        <b>Environment Variables</b>
        There are two types of metrics in the Q&T solution package: reference based and reference free metrics. Reference based metrics need a ground truth against which to evaluate the application response. Reference free metrics in the Q&T solution are evaluated using LLMs. These metrics align with human expectations and can be computed using any LLM. An LLM is a Judge for evaluation of these non-ground truth based/ reference free metrics. The user can switch between different models or even open source LLMs by providing the model details. Following are the methods to set up / configure the models and to setup environment variables.
        
        <b>For Azure OpenAI model:</b>
        Run the following commands to configure your environment to use AzureOpenAI models for all LLM-based metrics. To use AzureOpenAI models for evaluation, supply the model details as below:
        • os.environ["MODEL_TYPE"] = "azure"
        • os.environ["AZURE_OPENAI_API_KEY"] = "1213jaj...." 
        • os.environ["Azure_OPENAI_VERSION"] = ""
        • os.environ["AZURE_API_BASE_URL"] = ""
        • os.environ["AZURE_MODEL_DEPLOYMENT_NAME"] ="davinci"

        <b>For OpenAI models :</b> 
        • os.environ["MODEL_TYPE"] = "openai"
        • os.environ["OPENAI_API_KEY"] = "sk-..."
        • os.environ["OPENAI_MODEL_NAME"] = ""

        <b>For Anthropic Claude models:</b>
        • os.environ["MODEL_TYPE"] = "claude"
        • os.environ["ANTHROPIC_API_KEY"] = "sk-..."
        • os.environ["CLAUDE_MODEL_NAME"] = ""

        <b>For Mistral models:</b>
        • os.environ["MODEL_TYPE"] = "mistral"
        • os.environ["MISTRAL_API_KEY"] = "sk-..."
        • os.environ["MISTRAL_MODEL_NAME"] = ""

        <b>For Ollama models:</b> 
          Run the following commands to configure your environment to use an open source model for all LLM-based metrics. Quantized models can be used on the CPU without high latency/ without increasing computational costs. Some models that you can try are: Llama 3.1, MS phi3.5 mini etc
        • os.environ["MODEL_TYPE"] = "ollama"
        • os.environ["OLLAMA_MODEL_NAME"] = ""

        <b>For AWS Bedrock models:</b>
        • os.environ["MODEL_TYPE"] = "bedrock"
        • os.environ["AWS_ACCESS_KEY_ID"] = ""
        • os.environ["AWS_SECRET_ACCESS_KEY"] = ""
        • os.environ["AWS_REGION_NAME"] = ""
        • os.environ["BEDROCK_MODEL_NAME"] = ""
      </div>

      <div id="dbConf">
        To integrate Q&T with any application, we need database to store evaluation scores that can be visualized in dashboard. Hence, we need to set environment variables to configure
 
        <b>For mysql database:</b>
        • os.environ["DB_TYPE"] = "mysql"
        • os.environ["MYSQL_USER"] = ""
        • os.environ["MYSQL_PASSWORD"] = ""
        • os.environ["MYSQL_HOST"] = ""
        • os.environ["MYSQL_PORT"] = "3306"
        • os.environ["MYSQL_DB_NAME"] = ""
        
        <b>For postgres database:</b>
        • os.environ["DB_TYPE"] = "postgres"
        • os.environ["POSTGRES_USER"] = ""
        • os.environ["POSTGRES_PASSWORD"] = ""
        • os.environ["POSTGRES_HOST"] = ""
        • os.environ["POSTGRES_PORT"] = "5432"
        • os.environ["POSTGRES_NAME"] = ""

        <b>For mssql database:</b>
        • os.environ["DB_TYPE"] = "mssql"
        • os.environ["MSSQL_USER"] = ""
        • os.environ["MSSQL_PASSWORD"] = ""
        • os.environ["MSSQL_HOST"] = ""
        • os.environ["MSSQL_PORT"] = "1433"
        • os.environ["MSSQL_NAME"] = ""

        <b>For oracle database :</b>
        • os.environ["DB_TYPE"] = "oracle"
        • os.environ["ORACLE_USER"] = ""
        • os.environ["ORACLE_PASSWORD"] = ""
        • os.environ["ORACLE_HOST"] = ""
        • os.environ["ORACLE_PORT"] = "1521"
        • os.environ["ORACLE_NAME"] = ""

        
        <b>For databricks database :</b>
        • os.environ["DB_TYPE"] = "delta_table"
        • os.environ["DATABRICKS_HTTP_PATH"] = ""
        • os.environ["DATABRICKS_ACCESS_TOKEN"] = ""
        • os.environ["DATABRICKS_HOST"] = ""
        • os.environ["DATABRICKS_CATALOG"] = ""
        • os.environ["DATABRICKS_SCHEMA"] = ""

        If none of the above databases is used, by default sqlite database is used and quality_trust.db is created.
      </div>

      <div id="setDb">
        <b>How to setup database</b>
 
        • Install any of the database tools like postgres, MySQL workbench/server, MS SQL, Oracle etc. 
        
        • Setup credentials in the tool and accordingly change the database configuration (link of db config)
        
        • Create a database in database tool and update database name in database configuration (link)
        
        • To create schema, follow these steps - 
        
        <b>Steps to create schema</b>
        Run the following python commands -  
        
        <b>#Import below library from qualitytrust</b>
        from qualitytrust import model
                  
        <b>#Execute the following command</b>
        model.create_schema()

        
        <b>Insert metadata in database</b>
        With this metadata following tables will be loaded -
                    applications
                    metric_type
                    metrics
                    metric_mapping
        
        Format of the schema which is provided explicitly should be as below –
        
        - application: name_of_the_application
                    app_description: GenAI tool which assist clinical reviewers in decision support through infornation retrieval, extraction and accessing medical necessity of requested services
          mapping:
            - metric_type: Application
              metrics:
                - metric_class: A.F.T.R
                  description: ""
                  task: All
                  metric_name: A.F.T.R
                  soft_threshold: 0.7
                  hard_threshold: 0.4
        
        This data should be in metric_config.yml file as per the requirements.
        
        <b>Logging metadata in database</b>
        Use the following commands to log metadata into the database using qualitytrust package
        
        <b>#Import log metadata library </b>
        from app.qualitytrust.suite.log_metadata import LogMetadata
        
        <b>#Run the python commands</b>
        log_metadata = LogMetadata()
        log_metadata.log(metadata_path="app/qualitytrust/config/metric_config.yml")

      </div>
      
      <div id="response">
        <table> <tr> <th>Name</th> <th>Definition</th> </tr> <tr> <th>Accuracy</th> <td></td> </tr> <tr> <td>Exact Match</td> <td>Exact match is the proportion of the predicted output that matches the reference.</td> </tr> <tr> <td>Rouge Score </td> <td>Rouge Score compares the overlap of words or phrases (n-grams) between the generated answer and reference answer.</td> </tr> <tr> <td>Factual Accuracy</td> <td>Factual accuracy grades how factual the generated response was. </td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Relevancy</th> <td></td> </tr> <tr> <td>Reponse Relevance</td> <td>Measures how relevant the generated response was to the question specified and irrelevant additional information.</td> </tr> <tr> <td>Text Relevance</td> <td>Evaluates relevance between prompts and responses by computing similarity scores between embeddings generated from prompts and responses.</td> </tr> <tr> <td>Reponse Completeness</td> <td>Grades whether the response has answered all the aspects of the question specified.</td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Hallucination</th> <td></td> </tr> <tr> <td>Hallucination Degree</td> <td>Grades how concise the generated response is ie. the extent of additional irrelevant information in the response.</td> </tr> <tr> <td>Context Validity</td> <td>Evaluates how relevant the retrieved context is to the question specified.</td> </tr> <tr> <td>Context Utilization </td> <td>Context Utilization score measures if the generated response has sufficiently used the retrieved context to answer the question being asked.</td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Robustness</th> <td></td> </tr> <tr> <td>Syntax Sensitivity</td> <td>This class of metrics assesses the NLP model&#39;s ability to handle input text that includes abbreviations and other changes to language. </td> </tr> <tr> <td>Language Critique</td> <td>The Language Critique metric - scores machine generated response on multiple aspects, fluency, politeness, grammar, and coherence. </td> </tr> <tr> <td>Aspect Critique</td> <td>This metric is designed to assess submissions based on predefined aspects such as harmlessness and correctness.</td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Efficiency</th> <td></td> </tr> <tr> <td>Latency </td> <td>Measures the time it takes for an LLM to generate a response. </td> </tr> <tr> <td>Cost </td> <td>The cost metric measures the token cost of LLM response.</td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Bias</th> <td></td> </tr> <tr> <td>Gender and Racial Bias</td> <td>The bias metric determines whether the LLM output contains gender, or racial bias.</td> </tr> <tr> <td>Honest Score</td> <td>The Honest score aims to measure hurtful sentence completions in language models. </td> </tr> <tr> <td></td> <td></td> </tr> <tr> <th>Toxicity</th> <td></td> </tr> <tr> <td>Toxic Opinions</td> <td>Evaluates toxicity in the application response includes personal attack, mockery, threats or intimidation. </td> </tr> <tr> <td>Abusive Speech </td> <td>Quantifies abusive speech targeting ethnicity, religion or gender in the input text using a pretrained hate speech classification model.</td> </tr> </table>
      </div>
    </div>
    <!-- JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="script.js"></script>
  </body>
</html>
