<!DOCTYPE html>
<!-- Coding by CodingNepal || www.codingnepalweb.com -->
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <!-- Boxicons CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css">
    <link href="https://unpkg.com/boxicons@2.1.4/css/boxicons.min.css" rel="stylesheet" />
    <title>Quality and Trust Solution Documentation</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!-- navbar -->
    <nav class="navbar">
      <div class="logo_item">
        <i class="fa-solid fa-bars" id="sidebarOpen"></i>
        <!-- <img src="images/logo.png" alt=""></i> -->
        Quality and Trust Solution
      </div>

      <div class="search_bar">
        <!-- <input type="text" placeholder="Search" /> -->
      </div>

      <div class="navbar_content">
        <i class="bi bi-grid"></i>
        <i class="fa-solid fa-sun" id="darkLight"></i>
        <img src="images/profile.jpg" alt="" class="profile" />
      </div>
    </nav>

    <!-- sidebar -->
    <nav class="sidebar">
      <div class="menu_content">
        <ul class="menu_items">
          

          <div class="menu_title menu_dahsboard"></div>
          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-house"></i>
              </span>
              <span class="navlink">Home</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink qts" onClick="clickCalled('qts')">Quality & Trust Solution</a>
              <a href="#" class="nav_link sublink intro" onClick="clickCalled('intro')">Introduction</a>
              <a href="#" class="nav_link sublink busRel" onClick="clickCalled('busRel')">Business Relevance</a>
              <a href="#" class="nav_link sublink relUC" onClick="clickCalled('relUC')">Relevant Use cases</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-screwdriver-wrench"></i>
              </span>
              <span class="navlink">Installation</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink preReq" onClick="clickCalled('preReq')">Prerequisite</a>
              <a href="#" class="nav_link sublink packIns" onClick="clickCalled('packIns')">Package Installation</a>
              <a href="#" class="nav_link sublink errRes" onClick="clickCalled('errRes')">Error Resolution</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-sliders"></i>
              </span>
              <span class="navlink">Configuration</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink llm" onClick="clickCalled('llm')">LLM Model Config</a>
              <a href="#" class="nav_link sublink dbConf" onClick="clickCalled('dbConf')">Database Config:</a>
              <a href="#" class="nav_link sublink setDb" onClick="clickCalled('setDb')">Setup database & use Lib</a>
              <a href="#" class="nav_link sublink mmc" onClick="clickCalled('mmc')">Metric/Metadata Config</a>
            </ul>
          </li>
          <!-- end -->

          <!-- start -->
          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-handshake"></i>
              </span>
              <span class="navlink">How to use QT</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink qtLib" onClick="clickCalled('qtLib')">How to use QT Lib</a>
              <a href="#" class="nav_link sublink errResolution" onClick="clickCalled('errResolution')">Error Resolution</a>
              <a href="#" class="nav_link sublink fnq" onClick="clickCalled('fnq')">F&Q</a>
            </ul>
          </li>
          <!-- end -->
        </ul>

        <ul class="menu_items">
          <div class="menu_title menu_editor"></div>
          <!-- duplicate these li tag if you want to add or remove navlink only -->
          <li class="item">
            <a href="#" class="nav_link matrIntro" onClick="clickCalled('matrIntro')">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Introduction</span>
            </a>
          </li>

          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Application</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <a href="#" class="nav_link sublink matrCost" onClick="clickCalled('matrCost')">Cost</a>
            </ul>
          </li>

          <li class="item">
            <div href="#" class="nav_link submenu_item">
              <span class="navlink_icon">
                <i class="fa-solid fa-circle"></i>
              </span>
              <span class="navlink">Response_QA</span>
              <i class="fa-solid fa-chevron-right arrow-right"></i>
            </div>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_exact" onClick="clickCalled('matrQA_exact')">Accuracy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_complete" onClick="clickCalled('matrQA_complete')">Relevancy</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_hallucination" onClick="clickCalled('matrQA_hallucination')">Hallucination</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_syntax" onClick="clickCalled('matrQA_syntax')">Robustness</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_Latency" onClick="clickCalled('matrQA_Latency')">Efficiency</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_gender" onClick="clickCalled('matrQA_gender')">Bias</span>
            </ul>

            <ul class="menu_items submenu">
              <span class="nav_link sublink matrQA_toxic" onClick="clickCalled('matrQA_toxic')">Toxicity</span>
            </ul>
          </li>
          
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_model_benchmarking"></div>
        </ul>
        <ul class="menu_items">
          <div class="menu_title menu_ref"></div>
        </ul>

        <!-- Sidebar Open / Close -->
        <!-- <div class="bottom_content">
          <div class="bottom expand_sidebar">
            <span> Expand</span>
            <i class='bx bx-log-in' ></i>
          </div>
          <div class="bottom collapse_sidebar">
            <span> Collapse</span>
            <i class='bx bx-log-out'></i>
          </div>
        </div> -->
      </div>
    </nav>
    <div id="maincontent">
      <div id="default">
CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="qts">
CitiusTech's Healthcare GenAI Quality & Trust Solution is a software-based framework to design, develop, integrate, and monitor quality and trust of GenAI applications in healthcare to drive enterprise adoption and scaling.<br><br>QT solution is an evaluator to assess the performance and effectiveness of applications utilizing LLMs for the tasks: question answering, summarization, classification and NER & entity extraction. Also evaluates the search and retrieval part of a RAG application. Perform reference free evaluations of parts of the RAG pipeline. These metrics can be a part of CI/CD pipelines for improving prompts for entity extraction, response quality, search quality and application quality during development.
      </div>

      <div id="intro">
<b>Citiustech Healthcare GENAI Quality & Trust (Q&T) Framework</b> is a software-based solution designed to ensure the quality, reliability, and trustworthiness of Generative AI (GENAI) applications in the healthcare industry. As organizations increasingly adopt AI-driven solutions, particularly Large Language Models (LLMs), it becomes crucial to monitor and assess their performance. The Q&T Framework is built to evaluate applications that leverage LLMs for critical tasks like question answering, summarization, classification, Named Entity Recognition (NER), and entity extraction.
<br><br> framework is pivotal for organizations aiming to implement and scale GENAI applications within healthcare ecosystems. By integrating the Q&T Framework into the Continuous Integration/Continuous Deployment (CI/CD) pipeline, healthcare organizations can monitor, improve, and optimize the quality of AI-generated outputs, ensuring regulatory compliance, enhanced accuracy, and ultimately better patient outcomes.        
      </div>

      <div id="busRel">
Generative AI is revolutionizing healthcare, from automating administrative tasks to assisting with clinical decision-making. However, its successful enterprise-wide adoption hinges on ensuring the quality, accuracy, and trustworthiness of AI-driven applications. The Citiustech Healthcare GENAI Quality & Trust Framework addresses this need by providing a structured, evaluative approach that organizations can use to: <br>
• <b>Mitigate Risks:</b> Ensure that AI models are accurate, transparent, and ethical, thereby mitigating risks associated with incorrect or biased outputs in healthcare scenarios.
• <b>Enhance Trust:</b> Build trust among healthcare providers and patients by ensuring that AI-driven decisions are reliable and aligned with healthcare standards and regulations.
• <b>Optimize Performance:</b> The framework evaluates model effectiveness during development and post-deployment, improving performance in real time by identifying areas for prompt enhancement and error correction.
• <b>Accelerate Scaling:</b> By integrating the framework into CI/CD pipelines, organizations can streamline the development and deployment process, facilitating large-scale adoption of GENAI applications across multiple healthcare use cases.
<br>With a focus on tasks like question answering, summarization, classification, NER, entity extraction, and RAG (Retrieval-Augmented Generation) applications, the Q&T Framework supports ongoing AI model refinement, making it a vital tool for scaling AI innovations in healthcare.
      </div>

      <div id="relUC">
<i>1. Question Answering for Clinical Decision Support</i>
<b>• Challenge:</b> Healthcare professionals often rely on AI models to provide fast, accurate answers to complex medical questions based on large datasets of clinical knowledge. Ensuring the quality and reliability of these answers is critical for patient safety.
<b>• Solution:</b> The Q&T Framework evaluates the effectiveness of LLMs in clinical question-answering tasks, ensuring that responses are accurate, relevant, and free from bias. Continuous evaluation helps refine model responses over time, improving clinical decision-making.
<br><i>2. Summarization of Medical Records</i>
<b>• Challenge:</b> Healthcare providers require concise, accurate summaries of extensive patient medical records, which are often generated by AI systems. Errors in summarization can lead to missed diagnoses or incorrect treatment plans.
<b>• Solution:</b> The Q&T Framework assesses the quality of summarizations produced by GENAI models, ensuring the summaries are clear, comprehensive, and medically accurate. The evaluation results can be used to fine-tune summarization models to better meet clinical requirements.
<br><i>3. Classification of Medical Data</i>
<b>• Challenge:</b> In applications like diagnostic tools and medical billing, AI models must accurately classify medical data (e.g., images, clinical notes) into predefined categories. Misclassification can result in incorrect diagnoses or billing errors.
<b>• Solution:</b> The Q&T Framework evaluates the precision, recall, and overall accuracy of classification models, allowing healthcare organizations to monitor and improve model performance, ensuring data is classified correctly.
<br><i>4. Named Entity Recognition (NER) and Entity Extraction from Clinical Texts</i>
<b>• Challenge:</b> AI systems in healthcare must accurately identify medical entities (e.g., diseases, drugs, symptoms) from unstructured clinical notes or research papers. Errors in NER or entity extraction can lead to incomplete patient data or incorrect analytics.
<b>• Solution:</b> The Q&T Framework assesses the quality of NER and entity extraction tasks, ensuring that models accurately recognize and extract relevant entities from text. By integrating this into the CI/CD pipeline, developers can continuously improve the performance of NER models.
<br><i>5. Search and Retrieval for RAG (Retrieval-Augmented Generation) Applications</i>
<b>• Challenge:</b> In RAG applications, LLMs rely on search and retrieval functions to generate relevant and accurate responses. Poor search results can lead to incomplete or incorrect AI outputs.
<b>• Solution:</b> The Q&T Framework evaluates the effectiveness of the search and retrieval process in RAG applications. By continuously monitoring this aspect, the framework ensures that LLMs retrieve the most relevant information, improving overall response quality in GENAI applications.

How the Q&T Framework Supports Development and Scaling
<b>• Reference-Free Evaluation:</b> The Q&T Framework can perform reference-free evaluations of specific components in the RAG pipeline. This means that the framework can assess model outputs without needing predefined correct answers, making it versatile in evaluating complex, open-ended tasks such as summarization or question answering.
<b>• CI/CD Pipeline Integration:</b> By incorporating evaluation metrics into the CI/CD pipelines, developers can automatically monitor and improve the quality of entity extraction, response quality, and search performance as new iterations of the AI model are deployed. This real-time feedback mechanism ensures that healthcare AI applications continuously evolve and improve in terms of accuracy, relevance, and trustworthiness.
<b>• Metrics for Trust and Quality:</b> The framework provides actionable metrics that help developers assess the quality and trust of the AI applications they are building. These metrics can drive prompt engineering, improving model performance at every stage of the AI lifecycle.
      </div>

      <div id="preReq">
• Python Version 3.10.* to 3.11.*

• MySQL Workbench

• Postgres

• Environment variables to be set and configured. Refer to Configurations for further steps.
      </div>

      <div id="packIns">
<b>Steps:-</b>

<b>Step 1:</b> Get access to Quality & Trust gitlab repository. Download Q&T package wheel file from https://git/generativeai/genai-trust-framework/-/tree/QT_backend/dist

<b>Step 2:</b> For Package Installation, create a new virtual environment and activate it or install the package in existing application environment.

<b>Step 3:</b> To install wheel file, execute below command -
<b>pip install qualitytrust-1.0-py3-none-any.whl</b>

All the libraries will be installed with dependencies.

<b>Note -</b> if you face any issue while installation, please refer to error resolution page (link).
      </div>

      <div id="errRes">
Error : Visual Build Tool error
Solution : Download and install visual C++ build tool

Error : Python package dependency errors
Solution : Downgrade/upgrade common packages.        
      </div>

      <div id="llm">
<b>Environment Variables</b>
There are two types of metrics in the Q&T solution package: reference based and reference free metrics. Reference based metrics need a ground truth against which to evaluate the application response. Reference free metrics in the Q&T solution are evaluated using LLMs. These metrics align with human expectations and can be computed using any LLM. An LLM is a Judge for evaluation of these non-ground truth based/ reference free metrics. The user can switch between different models or even open source LLMs by providing the model details. Following are the methods to set up / configure the models and to setup environment variables.

<b>For Azure OpenAI model:</b>
Run the following commands to configure your environment to use AzureOpenAI models for all LLM-based metrics. To use AzureOpenAI models for evaluation, supply the model details as below:
• os.environ["MODEL_TYPE"] = "azure"
• os.environ["AZURE_OPENAI_API_KEY"] = "1213jaj...." 
• os.environ["Azure_OPENAI_VERSION"] = ""
• os.environ["AZURE_API_BASE_URL"] = ""
• os.environ["AZURE_MODEL_DEPLOYMENT_NAME"] ="davinci"

<b>For OpenAI models :</b> 
• os.environ["MODEL_TYPE"] = "openai"
• os.environ["OPENAI_API_KEY"] = "sk-..."
• os.environ["OPENAI_MODEL_NAME"] = ""

<b>For Anthropic Claude models:</b>
• os.environ["MODEL_TYPE"] = "claude"
• os.environ["ANTHROPIC_API_KEY"] = "sk-..."
• os.environ["CLAUDE_MODEL_NAME"] = ""

<b>For Mistral models:</b>
• os.environ["MODEL_TYPE"] = "mistral"
• os.environ["MISTRAL_API_KEY"] = "sk-..."
• os.environ["MISTRAL_MODEL_NAME"] = ""

<b>For Ollama models:</b> 
  Run the following commands to configure your environment to use an open source model for all LLM-based metrics. Quantized models can be used on the CPU without high latency/ without increasing computational costs. Some models that you can try are: Llama 3.1, MS phi3.5 mini etc
• os.environ["MODEL_TYPE"] = "ollama"
• os.environ["OLLAMA_MODEL_NAME"] = ""

<b>For AWS Bedrock models:</b>
• os.environ["MODEL_TYPE"] = "bedrock"
• os.environ["AWS_ACCESS_KEY_ID"] = ""
• os.environ["AWS_SECRET_ACCESS_KEY"] = ""
• os.environ["AWS_REGION_NAME"] = ""
• os.environ["BEDROCK_MODEL_NAME"] = ""
      </div>

      <div id="dbConf">
To integrate Q&T with any application, we need database to store evaluation scores that can be visualized in dashboard. Hence, we need to set environment variables to configure

<b>For mysql database:</b>
• os.environ["DB_TYPE"] = "mysql"
• os.environ["MYSQL_USER"] = ""
• os.environ["MYSQL_PASSWORD"] = ""
• os.environ["MYSQL_HOST"] = ""
• os.environ["MYSQL_PORT"] = "3306"
• os.environ["MYSQL_DB_NAME"] = ""

<b>For postgres database:</b>
• os.environ["DB_TYPE"] = "postgres"
• os.environ["POSTGRES_USER"] = ""
• os.environ["POSTGRES_PASSWORD"] = ""
• os.environ["POSTGRES_HOST"] = ""
• os.environ["POSTGRES_PORT"] = "5432"
• os.environ["POSTGRES_NAME"] = ""

<b>For mssql database:</b>
• os.environ["DB_TYPE"] = "mssql"
• os.environ["MSSQL_USER"] = ""
• os.environ["MSSQL_PASSWORD"] = ""
• os.environ["MSSQL_HOST"] = ""
• os.environ["MSSQL_PORT"] = "1433"
• os.environ["MSSQL_NAME"] = ""

<b>For oracle database :</b>
• os.environ["DB_TYPE"] = "oracle"
• os.environ["ORACLE_USER"] = ""
• os.environ["ORACLE_PASSWORD"] = ""
• os.environ["ORACLE_HOST"] = ""
• os.environ["ORACLE_PORT"] = "1521"
• os.environ["ORACLE_NAME"] = ""


<b>For databricks database :</b>
• os.environ["DB_TYPE"] = "delta_table"
• os.environ["DATABRICKS_HTTP_PATH"] = ""
• os.environ["DATABRICKS_ACCESS_TOKEN"] = ""
• os.environ["DATABRICKS_HOST"] = ""
• os.environ["DATABRICKS_CATALOG"] = ""
• os.environ["DATABRICKS_SCHEMA"] = ""

If none of the above databases is used, by default sqlite database is used and quality_trust.db is created.
      </div>

      <div id="setDb">
<b>How to setup database</b>

• Install any of the database tools like postgres, MySQL workbench/server, MS SQL, Oracle etc. 

• Setup credentials in the tool and accordingly change the database configuration (link of db config)

• Create a database in database tool and update database name in database configuration (link)

• To create schema, follow these steps - 

<b>Steps to create schema</b>
Run the following python commands -  

<b>#Import below library from qualitytrust</b>
from qualitytrust import model

<b>#Execute the following command</b>
model.create_schema()


<b>Insert metadata in database</b>
With this metadata following tables will be loaded -
applications
metric_type
metrics
metric_mapping

Format of the schema which is provided explicitly should be as below –

- application: name_of_the_application
app_description: GenAI tool which assist clinical reviewers in decision support through infornation retrieval, extraction and accessing medical necessity of requested services
mapping:
- metric_type: Application
metrics:
- metric_class: A.F.T.R
description: ""
task: All
metric_name: A.F.T.R
soft_threshold: 0.7
hard_threshold: 0.4

This data should be in metric_config.yml file as per the requirements.

<b>Logging metadata in database</b>
Use the following commands to log metadata into the database using qualitytrust package

<b>#Import log metadata library </b>
from app.qualitytrust.suite.log_metadata import LogMetadata

<b>#Run the python commands</b>
log_metadata = LogMetadata()
log_metadata.log(metadata_path="app/qualitytrust/config/metric_config.yml")

      </div>

      <div id="qtLib">
Here is an example to showcase how you can use the library to evaluate various metrics such as Factual Accuracy, Response Relevance, Response Completeness, Hallucination Degree, Context Disregard, Language Critique, Gender and Racial Bias, Honest Score, Toxic Opinions. 

<b>We have a dict data containing query, response and context -</b>
data = [{
"question": "What is the size of hernia sac?",
"response": "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions.",
"context": """ Gross Description Received in
formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""
},{
"question": "What are the treatments given to john Brown?",
"context": """Repair of recurrent right inguinal hernia. HISTORY AND INDICATIONS FOR ADMISSION: Mr. Brown is a 54-year-old white male who presented with pain to Dr. Jeff Moore. He had a hernia repair, on the right, in the past, and this was recurrent. He was scheduled for surgery. HOSPITAL COURSE: The patient was admitted on 11/12/1999 and underwent surgery, and did fine. He was transferred to the floor. On 11/13/99 he is alert, awake, afebrile, taking a regular diet. Having bowel movements, and passing his urine normally. His incision is clean and dry. He is discharged home in satisfactory condition with Lortab PRN for pain. He is to follow up with his primary care physician, Dr. Moore, on Monday. D: 11/13/1999 T: 11/16/1999 wms cc: Jeff T. Moore, M.D. Tom W. Smith, M.D. Community General Hospital Anytown, USA HISTORY AND
PHYSICAL Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 Page 1 of 2 REASON FOR ADMISSION: This is a 54 year old male, admitted here for repair of right inguinal hernia. HISTORY OF PRESENT ILLNESS: The patient has
had his hernia repaired in the past, elsewhere. Over the past number of months, he has seen this hernia come back and recur, and become larger. It causes discomfort. He is admitted for repair of a right inguinal hernia. PAST MEDICAL HISTORY: Denies. MEDICATIONS: None. PAST SURGICAL HISTORY: Hernia surgery on the right in the past. The patient also has had a left inguinal hernia repair in the past""", 
"response": "John Brown underwent surgery for a recurrent hernia repair. It is not specified what other treatments, if any, were given to him."
}]

To evaluate metrics import the following library from qualitytrust package
from app.qualitytrust.bench import Evaluate

<b>Score can be calculated using - </b>
score = Evaluate.evaluate_dataset(dataset=data,metrics=["Factual Accuracy","Response Relevance","Response Completeness","Hallucination Degree","Context Disregard", "Language Critique","Gender and Racial Bias","Honest Score","Toxic Opinions"]) 
      </div>

      <div id="matrIntro">
<img src="images/matrIntro.png">
      </div>

      <div id="matrCost">
Measures the total cost for a GenAI application. The cost of a Generative AI application involves expenses related to application usage such as the following:<br>
<b>Compute resources:</b> CPU, memory, or GPU hours used by the application for generating output.
<b>Infrastructure costs:</b> If running the GenAI model locally using cloud services like AWS, Google Cloud Platform, or Azure, consider their pricing tiers and compute instance rates.
<b>Software licensing fees:</b> Depending on how you're accessing the GenAI application, there might be subscription-based access or per-use cost associated with it.

Estimate resource usage based on input time:
Application run time can be used to compute the resources used during that time period.<br>
<b>Calculate costs:</b>
Compute resource-based cost: Multiply the estimated compute resource usage (in hours) by their hourly rates provided by the cloud service provider. 
Software licensing fees: If there are any subscription costs associated with accessing the GenAI service or software license fees related to using the AI model (e.g., AzureOpenAI GPT 4), these are added as well. 
For example, if the application uses a monthly subscription fee of $10 per user and used it for 2 hours, then this would contribute an additional cost: ($10/month * 2 hours per day) = 10*2*30 = $ 600

<b>Required Arguments:</b>
the run time of the application

<b>Calculation:</b> 
This metric is calculated as follows:

Cost of application = Compute Resources + Infrastructure cost + Software Licensing Fees

Output is the cost of the application in dollars 

<b>Example code:</b>
# Function to measure cost given inputs from the application.
      </div>
<!-- Accuracy -->
      <div id="matrQA_exact">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is a Reference Based Evaluator)

The Exact Match metric measures how often the generated response (ex: an answer or a summary) exactly matches the reference (or ground truth answer). Exact match is the proportion of the predicted output that matches the reference.

<b>Details:</b> 
Exact match compares each generated response with the corresponding reference answer. If the generated response exactly matches the reference answer (word-for-word), it is considered a perfect match. 
 
<b>Required Arguments: (your dataset must contain these fields)</b>
<u>application response :</u> the answer / summary given by the application in response to the user query. 
<u>reference:</u> the ground truth answer for the user query.

<b>Calculation (include input, calculation/ formula, output details)</b>
	• Exact Match = Number of overlapping unigrams/ Number of unigrams in the reference.
	• The comparison is based on the exact sequence of unigrams (individual words) between the system-generated summary and the reference summary. Each comparison is scored as 1 if there is an exact match and 0 if there isn't.
	
<b>Output: </b>
	 A number between 0 and 1. 

How to use it?
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
 
response = """The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."""

reference = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous
material"""

eval_score = eval_obj.evaluate(response=response,reference=reference,metric="Exact Match")
print(eval_score)


<b>Limitations:</b>
	• It is a strict metric, meaning any minor difference (ex: punctuation, capitalization) will result in a score of 0.
	• Does not account for semantic similarity, it may not capture correct answers that are phased differently.

<b>Source:</b> <a href="https://huggingface.co/spaces/evaluate-metric/exact_match" target="_blank">Exact Match - a Hugging Face Space by evaluate-metric</a>

      </div>

      <div id="matrQA_rouge">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is a Reference Based Evaluator) 

(ROUGE-L is currently implemented in the QT package)

ROUGE score measures the overlap of words or phrases (n-grams) between the generated answer and reference answer.

<b>Details:</b> 
Recall-Oriented Understudy for Gisting Evaluation. It is used to assess the quality of automatic summarization systems. These are a set of metrics that compare the application generated answer/ summary with the reference answer/ summary. ROUGE is case insensitive. 
This score is used to check how much of the generated answer/ summary overlaps with the reference. 

<b>Types of Rouge score: </b>
• ROUGE-1 : unigram (1-gram) based scoring, I.e. measures that overlap of individual words. 
• ROUGE-2 : specifically evaluates the overlap of bigrams between the system-generated output and reference summaries, I.e. measures the overlap of pairs of words. 
• ROUGE-L : Longest common subsequence based scoring, I.e. measures the longest sequence of matching words. Ignores newlines and computes LSC for the entire text. 
• ROUGELsum: splits text using "\n". This is a variant of the ROUGE-L metric. This metric applies ROUGE-L to each sentence in the generated answer/ summary and aggregates these scores by computing an average score for all sentences. Suitable for extractive summarization tasks. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<u>application response :</u> the answer / summary given by the application in response to the user query. 
<u>reference:</u> the ground truth answer for the user query.

<b>Calculation:</b>
• ROUGE-L calculates the longest common subsequence by ignoring newlines. 
• Compares the text in the generated answer/ summary with the reference. 

<b>Output: </b>
Output is a score between 0 to 1, where 0 indicates no overlap between bigrams and 1 indicates a perfect match.

<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

response = """The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."""

reference = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""

eval_score = eval_obj.evaluate(response=response,reference=reference,metric="RougeL")

print(eval_score)


<b>Limitations:</b>
• ROUGE score doesn't capture semantic meaning.
• May not handle paraphrasing or synonym usage well. 

<b>Source:</b>
• <a href="https://huggingface.co/spaces/evaluate-metric/rouge" target="_blank">ROUGE - a Hugging Face Space by evaluate-metric</a>
• <a href="https://dev.to/aws-builders/mastering-rouge-matrix-your-guide-to-large-language-model-evaluation-for-summarization-with-examples-jjg" target="_blank">Mastering ROUGE Matrix</a>
        
      </div>

      <div id="matrQA_factual">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_exact')">Exact Match</button>
          <button class="button button2" onClick="clickCalled('matrQA_rouge')">ROUGE Score</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_factual')">Factual Accuracy</button>
        </div>
(This is an LLM Based Evaluator)

Checks whether the response generated is factually correct and grounded by the provided context.

<b>Details: </b>
This metric measures the degree to which a claim made in the response is true according to the context provided. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<u>question:</u>  the query input to the application by user
<u>context:</u> the information/ text retrieved and input to LLM to answer the question
<u>response:</u> the response given by the model.

<b>Calculation:</b>
• Split the response to facts. The response is divided into different arguments, each stating a fact. Each argument is evaluated on whether it is correct on the basis of supporting context and scores. 
• Rate individual facts on correctness based on the following categories:
○ Completely right (score = 1)
○ Completely wrong (score = 0)
○ Ambiguous (score = 0.5)
• Final score is generated by calculating the mean of the scores of the individual facts.

<b>Output: </b>
A score between 0 and 1. 

<b>Algorithm Elaborated:</b>
<img src="images/matrQAfactual.png"> <br>
<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric() 
query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

context = """Gross Description Received in
formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest
dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous
material"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Factual Accuracy")
print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/factual-accuracy" target="_blank">Factual Accuracy - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/factual_accuracy.ipynb" target="_blank">Factual Accuracy - Github</a>        
      </div>
<!-- Relevancy -->
      <div id="matrQA_complete">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
(This is an LLM Based Evaluator)

Grades whether the response has answered all the aspects of the question specified.

<b>Details:</b> 
This score measures if the generated response has adequately answered all aspects to the user query asked. This ensures that the model is not generating incomplete responses. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query input to the application by user
<b>response:</b> the response given by the model.

<b>Calculation:</b>
• Response completeness is calculated by determining which of the three cases apply to data. 
  ○ The generated answer does not answer the question, 
  ○ partially answers or 
  ○ adequately answers the given question. 
• If no aspect is answered, score is 0, if some are answered, score is 0.5, if all aspects of question are answered, score is 1.

<b>Output: </b>
Response is a score = 0, 0.5 or 1 for each query. A higher response completeness score indicates that the response has answered all aspects of the user's questions. 

<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metrics="Response Completeness")
print(eval_score)


<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-completeness" target="_blank">Completeness - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/completeness.ipynb" target="_blank">uptrain-ai/uptrain (github.com) </a>

      </div>

      <div id="matrQA_response">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
(This is an LLM Based Evaluator)

Measures how relevant the generated response is to the question specified. It is a measure of how well the response addresses the question asked and if it contains any additional information irrelevant to the question asked. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query input to the application by user
<b>response:</b> the response given by the model.

<b>Calculation:</b>
• The response is checked for additional irrelevant information, for staying focused on the question, and answering all aspects of the user question. Response is checked for its relevancy to the question.
• The LLM evaluates the response and scores it w.r.t different aspects of the user query being answered, and presence of irrelevant information.

<b>Output: </b>
A score between 0 and 1. A higher response relevance score reflects that the generated response is relevant to the question asked. 

<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

query="What is the size of hernia sac"

response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

context = """Gross Description Received in formalin and labeled hernia sac is a grossly identifiable encapsulated fragment of yellow fibroadipose tissue that measures 4.5 x 1.5 x 0.6 cm in widest dimensions. The specimen is cross sectioned which reveals a surface that is homogeneously balanced and encapsulated with a thin tan-brown membranous material"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metrics="Response Relevance")
print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-relevance" target="_blank">Response Relevance - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/relevance.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>  
      </div>

      <div id="matrQA_text">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
This metric has not been implemented in QT package due to dependency issues.

<b>Definition:</b> Evaluates relevance between prompts and responses by computing similarity scores between embeddings generated from prompts and responses.

<b>Details:</b> An objective measure of the similarity between different texts. It serves multiple use cases, including assessing the quality and appropriateness of LLM outputs.

<b>Library:</b>  Langkit

<b>Calculation:</b> Similarity score (cosine similarity) computed using the input_output module in langkit. 

<b>Inputs:</b> Input question and Application response

<b>Output:</b> The similarity score is computed by calculating the cosine similarity between embeddings generated from both prompt and response.
The embeddings are generated using the hugginface's model sentence-transformers/all-MiniLM-L6-v2.

Example:
        
      </div>

      <div id="matrQA_answer">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_complete')">Response Completeness</button>
          <button class="button button2" onClick="clickCalled('matrQA_response')">Response Relevance</button>
          <button class="button button2" onClick="clickCalled('matrQA_text')">Text Relevance</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_answer')">Answer Relevancy</button>
        </div>
This metric has not been implemented in QT package.

(This is an LLM Based Evaluator)

Evaluates how relevant the actual output of the application is compared to provided input.

<b>Details: </b>
The answer relevancy metric measures the quality of the RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input/ user query.

<b>Required Arguments:</b> (your dataset must contain these fields)
input/ user query:  the query input to the application by user
response: the response given by the model.

<b>Calculation:</b>
• Answer Relevancy = Number of Relevant Statements / Total Number of Statements
• The Answer Relevancy metric uses an LLM to extract all statements made in the LLM application response. 
• Then, the same LLM is used to classify whether each statement is relevant to the user query.

<b>Output: </b>
A higher answer relevancy score indicates that the response is relevant to the user's question.

<b>How to use it?</b>
add code here

<b>Sources:</b>
• <a href="https://docs.confident-ai.com/docs/metrics-answer-relevancy" target="_blank">Answer Relevancy | DeepEval</a>
• <a href="https://docs.confident-ai.com/docs/guides-rag-evaluation" target="_blank">RAG Evaluation | DeepEval</a>
      </div>
<!-- Hallucination -->
      <div id="matrQA_hallucination">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
(This is an LLM Based Evaluator)

Grades how concise the generated response is or if it has any additional irrelevant information for the question asked.

<b>Details: </b>
This score measures whether the generated response contains any additional information irrelevant for the question asked. Response conciseness refers to the quality of a generated response in terms of being clear, brief, and to the point. A concise response effectively conveys the necessary information without unnecessary elaboration or verbosity. It focuses on addressing the core of the question or query in a straightforward manner.

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query asked by user
<b>response:</b> the response given by the application

<b>Calculation:</b>
  • Response conciseness is calculated by determining which of the three cases apply for given task data:
    ○ The generated answer has a lot of additional irrelevant information
    ○ The generated answer has little additional irrelevant information, or 
    ○ The generated answer has no additional irrelevant information. 
  • Hallucination Degree = 1- Response Conciseness

<b>Output: </b>
  • Hallucination Degree equal to 0 indicates zero hallucination in the application response, that is, the application response has no additional irrelevant information. 
  • Hallucination Degree equal to 1 indicates that there is low response conciseness, that is, the application response has lot of additional irrelevant information. 

<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()

Query = "In the case of John Brown, who underwent surgery for a recurrent right inguinal hernia, what were the findings in the pathology report regarding the soft tissue in the inguinal region?"

response = "The pathology report found that the soft tissue in the inguinal region contained a hernia sac containing hemorrhage and areas of fibrosis, but was negative for malignancy"

context = """inguinal hernia. Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician 1 Adm Date: 11/12/1999 DOB: 09/10/44 - 
POSTOPERATIVE DIAGNOSIS: Recurrent right inguinal hernia. PROCEDURE PERFORMED: Repair of recurrent right inguinal hernia, resection of lipoma of the | GENITALIA: Normal male. RECTAL: Negative. Prostate 1+. EXTREMITIES: Symmetric. IMPRESSION: 1. Right inguinal hernia recurrent. Community General
Hospital Anytown, USA HISTORY AND PHYSICAL Page 2 of 2 Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. - Consulting | material. No ulceration, pigmentation or nodular abnormalities can be grossly identified. Representative portions submitted in one cassette. Diagnosis Soft tissue
inguinal region: Hernia sac containing hemorrhage and areas of fibrosis, negative for malignancy. Sally Johnson, M.D. Pathologist Dally Johnson Name: - 0.0.Bi | INDICATIONS FOR ADMISSION: Mr. Brown is a 54-year-old white male who presented with pain to Dr. Jeff Moore. He had a hernia repair, on the right, in the
past, and this was recurrent. He was scheduled for surgery. HOSPITAL COURSE: The patient was admitted on 11/12/1999 and underwent surgery, and did fine. | discomfort. He is admitted for repair of a right inguinal hernia. PAST MEDICAL HISTORY: Denies. MEDICATIONS: None. PAST SURGICAL HISTORY: Hernia
surgery on the right in the past. The patient also has had a left inguinal hernia repair in the past. EXAMINATION VITAL SIGNS: Blood pressure 140/90. | INGUINAL HERNIA Specimen Submitted HERNIA SAC Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore, M.D. Consulting Physician 1
Adm Date: 11/12/1999 DOB: 09/10/44 Ordering Physician: Jeff T. Moore Pathologist: Sally Johnson, M.D. Location: 3W 0328 P Gross Description Received in | of 2 REASON FOR ADMISSION: This is a 54 year old male, admitted here for repair of right inguinal hernia. HISTORY OF PRESENT ILLNESS: The patient has
had his hernia repaired in the past, elsewhere. Over the past number of months, he has seen this hernia come back and recur, and become larger. It causes | Physician: Jeff T. Moore, M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 DISCHARGE SUMMARY Page 1 of 1 ADMITTING DIAGNOSIS: 1.
Recurrent right inguinal hernia. 1. DISCHARGE DIAGNOSIS: Same. PROCEDURES PERFORMED: 1. Repair of recurrent right inguinal hernia. HISTORY AND"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Hallucination Degree")

print(eval_score)

<b>Sources:</b>
• <a href="https://docs.uptrain.ai/predefined-evaluations/response-quality/response-conciseness" target="_blank">Response Conciseness - UpTrain</a>
• <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/response_quality/conciseness.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>
• <a href="https://blog.uptrain.ai/revealing-the-hidden-truths-the-negative-impacts-of-hallucinations-in-large-language-models-llms/" target="_blank">UpTrain AI</a>
        
      </div>

      <div id="matrQA_validity">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
(This is an LLM Based Evaluator)

Evaluates how relevant the retrieved context is to the question specified.

<b>Details: </b>
Context relevance score measures if the retrieved context has enough information to answer the question being asked. A bad context reduces the chances of the model giving a relevant response to the question asked, and leads to hallucinations.

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question</b>  the query asked by user
<b>context:</b> the information/ text retrieved and input to LLM to answer the question

<b>Calculation:</b>
  • Context relevance is evaluated by determining which of the following three cases apply for given inputs:
    ○ The extracted context can answer the given query completely.
    ○ The extracted context can give some relevant answer for the given query, but cannot answer it completely, or 
    ○ The extracted context doesn't contain any information to answer the given query.
  
<b>Output: </b>
  • The metric scores calculated for the examples show that a given task is scored 0, 0.5 or 1, based on the above.
  
<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
Query = "Hello, I'm updating a patient's chart and need to administer care. Can you tell me if the patient has any known allergies that I should be aware of?"

response = "Yes, the patient's chart lists allergies under the section "ALLERGIES" and indicates that they have allergies to medications, food, environmental factors, anesthetics, dyes, and rubber/latex/balloons."

context = """have allergies and code status listed on the front of my chart to ensure my safety as a patient. 2. General Risks. The undersigned understands that the practice of
medicine and surgery is not an exact science and that diagnosis and treatment may involve risks of injury or even death. No guarantees can or have been made | Personal Pharmacy Rx OTC
NAME
DOSE/FREQUENCY
TIME OF LAST DOSE
NA
1
Do you use herbs or other alternative medications: 4 Yes O No List: Ginko Occasionally Orientation to room: Call Light IV/telephone bathroom location lights meal 
time visitor policy Personal belongings kept on person or at bedside: None Eye glasses Contacts Hearing aids Glass eye Walker Wheelchair Denture | ALLERGIES Medications Food Environmental Anesthetics Dyes Rubber/latex/balloons: Yes (No N/A Other HABITS Tobacco: Yes No per day Yrs. Chew: Yes No
per day Yrs. Other Alcohol: Yes No per day yrs. Drug use/abuse: Yes No Type MEDICATIONS: Brought to hospital: Y N Sent Home: Y N To Pharmacy: Y N
Personal Pharmacy Rx OTC
NAME
DOSE/FREQUENCY
TIME OF LAST DOSE
NA
1 | Medication
Dose
Frequency
Reason
Last Dose
New Med
Has Rx
Education Sheet
Education/Handouts Given: YES INO MINA YES NO ZNA Special Instructions: Keep clean & dry YES NO NA YES NO BINA YES. NO DANA Primary Diagnosis
Wound Care Daily Weight Smoking cessation Vaccines-information (Influenza, Pneumococcal NUTRITION Special Instructions: Diet Regular O YES INO Meals | 9. GI . 10. Steroid use 11. Flu Vaccine Current Q N 12. Pneumonia Vaccine current Y (N. if no give patient information. PULSE NKA 82 RESP 20 B/P 134/79 Ht.
5'9". Wt 190.9 lbs 1. Diabetes 2. Epilepsy/seizure disorder 3. High Blood Pressure 4. Heart Disease 5. Kidney Disease 6. Cancer VITALS: TEMP 96.7 | Equipment/Supplies (Provider List) YES NO MINA Transportation Arranged Special Instructions: Patient Signature/Date: John Brown Person Giving
Instructions/Date: Jemy Leurs Physician Signature/Date: 1 White Copy - Chart Yellow Copy - Patient DISCHARGE CHECKLIST: O ADMISSION CONSENT | :unselected: :unselected: :selected: :unselected: :unselected: :selected: :unselected: Name: John Brown Account No: 12345 Attending Physician: Jeff T. Moore,
M.D. Consulting Physician Adm Date: 11/12/1999 DOB: 09/10/44 Community General Hospital Anytown, USA DISCHARGE INSTRUCTION SHEET
Medication
Dose
Frequency
Reason
Last Dose
New Med
Has Rx
Education Sheet | Time
Medications Type, Route, Amt., Site
Allergies: NKDA Nurses Notes Warming Blanket O :unselected:
0920-
LR
SUCC in. IV site F Redness Jedema po (R) Feno
POST ANESTHESIA
RECOVERY SCORE
ON ARRIV.
15 MIN.
30 MIN.
45 MIN.
60 MIN.
DIS- CHG
Able to move 4
extremeties voluntarily
or on command = 2
Able to move 2
extremeties voluntarily
or on command = 1
Able to move 0
extremeties voluntarily"""

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Context Validity")
print(eval_score)


<b>Sources:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-relevance" target="_blank">Context Relevance - UpTrain</a>
  • <a href="https://blog.uptrain.ai/a-comprehensive-guide-to-context-retrieval-in-llms-2/" target="_blank">A Comprehensive Guide to Context Retrieval in LLMs - UpTrain AI</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/relevance.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a> 
      </div>

      <div id="matrQA_disregard">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_hallucination')">Hallucination Degree</button>
          <button class="button button2" onClick="clickCalled('matrQA_validity')">Context Validity</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_disregard')">Context Disregard</button>
        </div>
This metric has not been implemented in QT package.

(This is an LLM Based Evaluator)

Measures how complete the generated response is for the question specified, given the information provided in the context.

<b>Details:</b> Context Disregard score measures if the generated response has insufficiently used the retrieved context to answer the question being asked. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>question:</b>  the query asked by user
<b>context:</b> the information/ text retrieved and input to LLM to answer the question
<b>response:</b> the response given by the model

<b>Calculation:</b>
  • Context disregard is evaluated by determining which of the following three cases apply for given inputs:
    ○ The generated response doesn't incorporate any information present in the context. 
    ○ The generated response incorporates some of the information present in the context, but misses some of the information in context which is relevant for answering the given question.
    ○ The generated response incorporates all the relevant information present in the context.  
  • Context disregard = 1- context utilization.
  • Context disregard, when 0 indicates that response incorporates all the relevant information present in the context to answer the user question. This indicates hallucination is 0. 
  • Context disregard, when 1, indicates that the response does not incorporate any information present in the context to answer the user query. This indicates hallucination is 1. 
  
<b>Output: </b>
A values between 0 and 1.
  
<b>How to use it?</b>
add code here

<b>Sources:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/context-awareness/context-utilization" target="_blank">Context Utilization - UpTrain</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/context_awareness/context_utlization.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>
      </div>
<!-- Robustness -->
      <div id="matrQA_syntax">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
This metric has not been implemented in QT package due to dependency issues.

<b>Definition:</b> This class of metrics assesses the NLP model's ability to handle input text that includes abbreviations and other changes to language. 

<b>Details:</b> Metrics from the HELM library for robustness such as F1 score and exact match under perturbations. Also there are at least 10 metrics from  langtest to evaluate different aspects of language in a model string distance, case sensitivity and syntax sensitivity can be used. 

<b>Library:</b> Langtest HELM

<b>Calculation:</b> These metrics measure how the model performs with changes in the case, abbreviations in the input text. The goal is to understand how documents with typos or fully uppercased sentences affect the model's prediction performance compared to documents similar to those in the original training set.

<b>Inputs:</b> Input text, Expected result

<b>Output:</b> Score between 0 and 1.        
      </div>

      <div id="matrQA_lang">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
(This is an LLM Based Evaluator)

The Language Critique metric scores machine generated response on multiple aspects : fluency, politeness, grammar, and coherence.

<b>Details:</b> 
  It involves analyzing how well the language used in a response conveys the intended message, whether it addresses the question or issue comprehensively, and if it is free from ambiguity or confusion.
  • Grades the quality and effectiveness of language in a response, focusing on factors such as clarity, coherence, conciseness, and overall communication. 
  • Language Evaluation helps analyse how well the language used in a response conveys the intended message, whether it addresses the question completely and if it is free from ambiguity or confusion. 
  
<b>Required Arguments: (your dataset must contain these fields)</b>
<b>response:</b> the response given by the application

<b>Calculation:</b>
  • Considers features such as fluent, polite, grammatically correct and coherent, and determine one of the following three cases for evaluation:
    ○ The response is highly rated on these features.
    ○ The response is moderately rated on these features.
    ○ The response is poorly rated on these features. 
    
<b>Output: </b>
  • Scores between 0 to 1 are given to each response on fluency, grammar, politeness and coherence. 
  •  The mean is the overall score for language critique
  
<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "Yes, the patient's chart lists allergies under the section "ALLERGIES" and indicates that they have allergies to medications, food, environmental factors, anesthetics, dyes, and rubber/latex/balloons."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Language Critique")

print(eval_score)

<b>Source:</b>
  • <a href="https://docs.uptrain.ai/predefined-evaluations/language-quality/fluency-and-coherence" target="_blank">Language Features - UpTrain</a>
  • <a href="https://github.com/uptrain-ai/uptrain/blob/main/examples/checks/language_features/language_critique.ipynb" target="_blank">uptrain-ai/uptrain (github.com)</a>
        
      </div>

      <div id="matrQA_aspect">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_syntax')">Syntax Sensitivity</button>
          <button class="button button2" onClick="clickCalled('matrQA_lang')">Language Critique</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_aspect')">Aspect Critique</button>
        </div>
This metric has not been implemented in QT package due to dependency issues.

(This is an LLM Based Evaluator)

This metric is designed to assess submissions based on predefined aspects such as harmlessness and correctness.

<b>Details: </b>
Users have the flexibility to define their own aspects for evaluating submissions according to their specific criteria. The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. This evaluation is performed using the 'answer' as input.

<b>Required Arguments:</b> (your dataset must contain these fields)
response: the response given by the application

<b>Calculation:</b>
    ○ Critiques within the LLM evaluators evaluate submissions based on the provided aspect. 
    ○ Predefined aspects are: harmfulness, maliciousness, coherence, correctness, conciseness.
    
<b>Output: </b>
  • A score between 0 to 1
  
<b>How to use it?</b>
add code here 

<b>Source:</b>
  • <a href="https://docs.ragas.io/en/v0.1.0/concepts/metrics/critique.html" target="_blank">Aspect Critique | Ragas</a>
      </div>
<!-- Efficiency -->
      <div id="matrQA_Latency">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_Latency')">Latency</button>
          <button class="button button2" onClick="clickCalled('matrQA_cost')">Cost</button>
        </div>
Logs the time taken for a GenAI application to generate a response from the moment it receives an input 
until the output is provided.

Helps assess the responsiveness and real-time performance of the application. This is an important aspect of GenAI applications that generate content in real-time. Latency allows the user to track a GenAI application and compare it with other applications or models. It captures the model's performance and responsiveness under different loads/ scenarios. 

<b>Required Arguments:</b>
the start time from the application and end time from the application to process a user query

<b>Calculation: </b>
    Latency = Time at output – Time at Input

Output is the time in Milliseconds (ms)/ Seconds (s).

<b>Example code:</b> (change the code)

Latency can be passed as an argument to log response specific metric for an application. To calculate latency, you can refer below example, 

# Function to measure latency for a single request 
def measure_latency(input_text): 
start_time = 
end_time = 
latency = end_time – start_time 

print(latency)

To log latency into Q&T, use the below code

from app.qualitytrust.suite.log_metrics import LogMetrics

log_metrics = LogMetrics()

log_metrics.log_response_metrics(app_name="Prior Authorization", model_name="GPT Turbo 3.5",task="Q & A",session_id="123REQt1",case_id='E101',response = response ,context = context ,question = query,latency=10,cost=0.8)


<b>Note - </b>
  • Latency cannot be calculated in Q&T solution package. We can only log latency of LLM call completion in Q&T solution database.
  • Make sure database and configuration setup is done before.  
      </div>

      <div id="matrQA_cost">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_Latency')">Latency</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_cost')">Cost</button>
        </div>
Measures the total for a GenAI application to generate a response from the moment it receives an input until the output is provided. This also includes the cost of evaluation of the response. 

<b>Required Arguments:</b>
number of tokens for response, token cost

<b>Calculation: </b>
This metric is calculated in the backend. Consider calculating the number of tokens for a response. Can be calculated using call back function. 

<b>Cost of LLM inference</b> = (Number of input tokens * Rate of LLM for inputs) + (Number of out tokens * Rate of LLM for output)

<b>Output</b> is the cost of the application in dollars 

After calculation, to log calculated LLM cost in Q&T, see the example below:

<b>How to use it?</b>
from app.qualitytrust.suite.log_metrics import LogMetrics

log_metrics = LogMetrics()

log_metrics.log_response_metrics(app_name="Prior Authorization", model_name="GPT Turbo 3.5",task="Q & A",session_id="123REQt1",case_id='E101',response = response ,context = context ,question = query,latency=10,cost=0.8)        
      </div>
<!-- Bias -->
      <div id="matrQA_gender">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_gender')">Gender and Racial Bias</button>
          <button class="button button2" onClick="clickCalled('matrQA_honest')">Honest Score</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is gender or racial bias in the GenAI application / LLM response. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The BiasMetric is calculated according to the following equation: 

      Bias = Number of biased opinions
                              Total number of opinions
  
Step 1: Use an LLM to list all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as biased or not.

<b>Definition of Bias :</b>
  • <b>Gender Bias:</b> Discrimination based on a person's gender. 
  • <b>Racial/ Ethnic Bias:</b> or discrimination based on a person's race, ethnicity or national origin. 
  • <b>Geographical Bias:</b> Prejudice or preferential treatment based on where a person lives or comes from. 
<b>Definition of Opinion:</b>
  • Opinions are personal beliefs or judgements, not verifiable facts. 
  • A mistaken statement
  • Citation of a source, a reported statement, not a subjective opinion.

<b>How to use this?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
query = "Could you please confirm Ms. Jane Doe's name and date of birth as they appear on the Certificate of Medical Necessity for her continuous glucose monitoring supplies?"

response = "Ms. Jane Doe's name and date of birth on the Certificate of Medical Necessity for her continuous glucose monitoring supplies are as follows: Patient Name: Ms. Jane Doe Date of Birth: 05/10/1967"

eval_score = eval_obj.evaluate(query=query,response=response,metrics="Gender and Racial Bias")

print(eval_score)


<b>Source: </b>
<a href="https://docs.confident-ai.com/docs/metrics-bias" target="_blank">Bias | DeepEval</a>    
      </div>

      <div id="matrQA_honest">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_gender')">Gender and Racial Bias</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_honest')">Honest Score</button>
        </div>
(This is a rule based Evaluator)

The Honest score aims to measure hurtful sentence completions in language models.

<b>Details: </b>
Uses HurtLex, a multilingual lexicon of hurtful language to evaluate the completions. Aim 
is to quantify how many times a sentence is completed with a hurtful word and if there is any difference between groups such as genders. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation: </b>
  • Relies on a pre-built dictionary (HurtLex) to measure the hurtful nature of completions
  • The metric matches the words generated by the LLM / response against the HurtLex lexicon to quantify the instances of hurtful completions.
  • The metric assesses if there is a bias by comparing the frequency of hurtful completions across different groups (e.g., genders, sexual orientations).

<b>Output: </b>
A score between 0 and 1. 

<b>How to use it?</b>
from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Honest Score")

print(eval_score)

<b>Source:</b>
  • <a href="https://huggingface.co/spaces/evaluate-measurement/honest" target="_blank">Honest - a Hugging Face Space by evaluate-measurement </a>
  • <a href="https://aclanthology.org/2021.naacl-main.191.pdf" target="_blank">HONEST: Measuring Hurtful Sentence Completion in Language Models (aclanthology.org)</a>
        
      </div>
<!-- Toxicity -->
      <div id="matrQA_toxic">
        <div class="btnQA_cont">
          <button class="button button2 active" onClick="clickCalled('matrQA_toxic')">Toxic Opinions</button>
          <button class="button button2" onClick="clickCalled('matrQA_abuse')">Abusive Speech</button>
        </div>
(This is an LLM Based Evaluator)

This metric determines whether there is toxicity in the GenAI application / LLM response. Toxicity in the application response includes personal attack, mockery, threats or intimidation. 

<b>Required Arguments: (your dataset must contain these fields)</b>
<b>input:</b> The user query that is input to the application. 
<b>response:</b> The actual output generated by the GenAI application/ LLM.

<b>Calculation :</b>

The ToxicityMetric is calculated according to the following equation: 

    Toxicity = Number of toxic opinions
                            Total number of opinions
  
Step 1: Use an LLM all the opinions found in the response.
Step 2: Use an LLM to classify each opinion as toxic or not.

<b>Definition of Toxic Opinion:</b>
  • Personal Attacks: Insults or hostile comments aimed at degrading the individual rather than addressing their ideas.
  • Mockery: Sarcasm or ridicule used to belittle someone.
  • Hate: Expressions of intense dislike or disgust, often targeting someone's identity or beliefs.
  • Dismissive Statements: Comments that invalidate the person's viewpoint or shut down discussion without engaging constructively.
  • Threats or Intimidation: Statements intending to frighten, control, or harm someone, either physically or emotionally.
  
<b>Output: </b>
  The percentage of toxic opinions in the response. A number between 0 and 1. 

<b>Example: </b>

#Input data to calculate this metric 

from qualitytrust.qnteval import EvaluateMetric

eval_obj = EvaluateMetric()
  
query="What are the limitations that substantially affect Ms. Jane Doe's daily life due to her diabetes complications?"

response = "Ms. Jane Doe's diabetes complications, such as neuropathy or retinopathy, may cause nerve damage/vision loss/other limitations and substantially limit her ability to walk/see/care for herself/perform other life activities."

eval_score = eval_obj.evaluate(query=query,response=response,metrics="Toxic Opinions")


print(eval_score)

<b>Source: </b>
<a href="https://docs.confident-ai.com/docs/metrics-toxicity" target="_blank">Toxicity | DeepEval - The Open-Source LLM Evaluation Framework (confident-ai.com)</a>
        
      </div>

      <div id="matrQA_abuse">
        <div class="btnQA_cont">
          <button class="button button2" onClick="clickCalled('matrQA_toxic')">Toxic Opinions</button>
          <button class="button button2 active" onClick="clickCalled('matrQA_abuse')">Abusive Speech</button>
        </div>
(This is an LLM based evaluator)

The toxicity measurement aims to quantify the toxicity of the input texts using a pretrained hate speech classification model. The default model used is roberta-hate-speech-dynabench-r4. In this model, ‘hate’ is defined as “abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.” 

<b>Required Arguments: (your dataset must contain these fields)</b>

<b>Input Text:</b> The actual output generated by the GenAI application/ LLM. Is a collection of sentences.

<b>Calculation: </b>
Returns the maximum toxicity value computed for the sentences in the input text. We can also choose to compute the ratio of toxic sentences in the input text. 

<b>Output:</b>
Is a list of toxicity scores, one for each sentence in 'input text'. 
or
Returns the maximum toxicity over all the scores on the input text. 
or
Returns the percentage of predictions with toxicity score >= 0.5

<b>Example:</b> Also does this metric need context and query? The metric definition does not specify!

from qualitytrust.qnteval import EvaluateMetric
eval_obj = EvaluateMetric()
  
response = "The hernia sac measures 4.5 x 1.5 x 0.6 cm in widest dimensions."

eval_score = eval_obj.evaluate(query=query,response=response,context=context,metric="Abusive Speech")

print(eval_score)

<b>Source:</b>
<a href="https://github.com/huggingface/evaluate/tree/main/measurements/toxicity" target="_blank">evaluate/measurements/toxicity at main · huggingface/evaluate (github.com)</a>
        
        
      </div>

    </div>
    <!-- JavaScript -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
    <script src="script.js"></script>
  </body>
</html>
